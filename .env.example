# =============================================================================
# Babel Configuration (.env)
# =============================================================================
#
# Copy this file to .env and customize for your setup:
#   cp .env.example .env
#
# Then source it before using Babel:
#   source .env
#
# Or place .env in your project root for automatic loading (requires python-dotenv).
#
# =============================================================================
#
# ⚠️  SECURITY WARNING - READ BEFORE PROCEEDING
# =============================================================================
#
# NEVER COMMIT YOUR .env FILE TO VERSION CONTROL!
#
# Your .env file contains sensitive API keys and credentials. If committed:
#   - Keys are exposed in git history PERMANENTLY (even after deletion)
#   - Public repos are scanned by bots within minutes for leaked credentials
#   - Leaked keys can lead to unauthorized usage, billing attacks, data breaches
#   - Rotating compromised keys is disruptive and may not undo damage
#
# REQUIRED: Ensure .env is in your .gitignore:
#
#   echo ".env" >> .gitignore
#
# VERIFY before committing:
#
#   git status    # .env should NOT appear in untracked/modified files
#
# If you accidentally committed .env:
#   1. Immediately rotate ALL exposed API keys
#   2. Remove from history: git filter-branch or BFG Repo-Cleaner
#   3. Force push (coordinate with team if shared repo)
#
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Provider Selection
# -----------------------------------------------------------------------------
#
# Available providers:
#   - claude   : Anthropic Claude (requires ANTHROPIC_API_KEY)
#   - openai   : OpenAI GPT (requires OPENAI_API_KEY)
#   - gemini   : Google Gemini (requires GOOGLE_API_KEY)
#   - ollama   : Local LLM via Ollama (no API key needed)
#
# PRIORITY LOGIC:
#   If a remote API key is set, the remote provider will be used by default,
#   even if BABEL_LLM_PROVIDER=ollama is configured. This ensures that users
#   with API keys get the expected behavior while allowing local-only setups.
#
#   Priority order: Remote API key > Local LLM > Mock provider
#
# Default: claude (if ANTHROPIC_API_KEY is set)
#
# BABEL_LLM_PROVIDER=claude

# -----------------------------------------------------------------------------
# Remote LLM API Keys (choose one based on your provider)
# -----------------------------------------------------------------------------
#
# IMPORTANT: API keys are sensitive. Never commit them to version control.
# Only ONE provider API key is needed based on your BABEL_LLM_PROVIDER choice.
#

# Anthropic Claude API Key
# Get your key at: https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-api03-your-key-here

# OpenAI API Key
# Get your key at: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-your-key-here

# Google Gemini API Key
# Get your key at: https://makersuite.google.com/app/apikey
# GOOGLE_API_KEY=your-key-here

# -----------------------------------------------------------------------------
# Model Selection
# -----------------------------------------------------------------------------
#
# Override the default model for your chosen provider.
# If not set, uses the provider's default model.
#
# Claude models:
#   - claude-opus-4-1-20250414      (Large/Powerful)
#   - claude-opus-4-20250514        (Large/Powerful)
#   - claude-sonnet-4-20250514      (Balanced - default)
#   - claude-3-7-sonnet-20250219    (Lightweight)
#   - claude-3-5-haiku-20241022     (Cost-efficient)
#
# OpenAI models:
#   - gpt-5.2, gpt-5.2-pro          (Large/Powerful)
#   - gpt-5-mini                    (Balanced - default)
#   - gpt-5-nano                    (Cost-efficient)
#
# Gemini models:
#   - gemini-2.5-pro                (Large/Powerful)
#   - gemini-2.5-flash              (Balanced - default)
#   - gemini-2.5-flash-lite         (Cost-efficient)
#
# Ollama models (any locally installed model):
#   - llama3.2                      (default)
#   - mistral
#   - phi3
#   - codellama
#   - etc. (run 'ollama list' to see installed models)
#
# BABEL_LLM_MODEL=claude-sonnet-4-20250514

# -----------------------------------------------------------------------------
# Local LLM Configuration (Ollama)
# -----------------------------------------------------------------------------
#
# For local LLM usage with Ollama or compatible servers (LM Studio, LocalAI).
#
# To use local LLM:
#   1. Install Ollama: curl -fsSL https://ollama.com/install.sh | sh
#   2. Pull a model: ollama pull llama3.2
#   3. Start Ollama: ollama serve
#   4. Set provider: BABEL_LLM_PROVIDER=ollama
#
# Base URL for Ollama API (default: http://localhost:11434)
# Change this if using a different port or remote Ollama server.
#
# BABEL_LLM_BASE_URL=http://localhost:11434
#
# For LM Studio (OpenAI-compatible):
#   BABEL_LLM_PROVIDER=ollama
#   BABEL_LLM_BASE_URL=http://localhost:1234/v1
#
# For LocalAI:
#   BABEL_LLM_PROVIDER=ollama
#   BABEL_LLM_BASE_URL=http://localhost:8080/v1

# -----------------------------------------------------------------------------
# Project Path Configuration
# -----------------------------------------------------------------------------
#
# BABEL_PROJECT_PATH sets the default project directory for all babel commands.
# This allows running babel from any directory without losing project context.
#
# Priority order:
#   1. Explicit --project flag (highest)
#   2. BABEL_PROJECT_PATH environment variable
#   3. Current working directory (lowest)
#
# Why use this?
#   - Prevents accidental captures to wrong project when navigating directories
#   - Essential for development workflows where you cd between source folders
#   - Enables CI/CD pipelines to target specific projects
#
# Set to the absolute path of your project root (where .babel/ lives):
#
# BABEL_PROJECT_PATH=/absolute/path/to/your/project
#
# Example:
#   BABEL_PROJECT_PATH=/home/user/projects/myapp
#   BABEL_PROJECT_PATH=C:/Users/name/projects/myapp  (Windows)

# -----------------------------------------------------------------------------
# Example Configurations
# -----------------------------------------------------------------------------
#
# === Remote LLM (Claude - Recommended) ===
# BABEL_LLM_PROVIDER=claude
# ANTHROPIC_API_KEY=sk-ant-api03-your-key-here
# BABEL_LLM_MODEL=claude-sonnet-4-20250514
#
# === Remote LLM (OpenAI) ===
# BABEL_LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-your-key-here
# BABEL_LLM_MODEL=gpt-5-mini
#
# === Local LLM (Ollama - Privacy-focused) ===
# BABEL_LLM_PROVIDER=ollama
# BABEL_LLM_MODEL=llama3.2
# BABEL_LLM_BASE_URL=http://localhost:11434
#
# === Local LLM (LM Studio) ===
# BABEL_LLM_PROVIDER=ollama
# BABEL_LLM_MODEL=local-model
# BABEL_LLM_BASE_URL=http://localhost:1234/v1
#
# === Air-gapped Environment (No network) ===
# BABEL_LLM_PROVIDER=ollama
# BABEL_LLM_MODEL=mistral
# # No API keys needed - fully offline operation

# =============================================================================
# Your Configuration (uncomment and edit as needed)
# =============================================================================

# Provider (claude, openai, gemini, ollama)
# BABEL_LLM_PROVIDER=claude

# API Key (for remote providers)
# ANTHROPIC_API_KEY=

# Model override (optional)
# BABEL_LLM_MODEL=

# Base URL (for local LLM)
# BABEL_LLM_BASE_URL=

# Project root path (contains .babel/ folder)
# All babel commands will target this project regardless of current working directory.
# Set to absolute path where your .babel/ folder lives.
# BABEL_PROJECT_PATH=

# -----------------------------------------------------------------------------
# Parallelization Configuration
# -----------------------------------------------------------------------------
#
# Babel uses a task orchestrator for efficient parallel execution.
# These settings control worker pools, rate limits, and timeouts.
#
# The orchestrator automatically detects task types:
#   - I/O-bound (LLM calls, file operations) → ThreadPool
#   - CPU-bound (parsing, similarity) → ProcessPool
#
# Default behavior:
#   - Uses half of available CPU cores for CPU-bound work
#   - 4 threads for I/O-bound work
#   - 3 concurrent LLM API calls (respects rate limits)
#

# Enable/disable parallelization (default: true)
# Set to false to force sequential execution (useful for debugging).
# BABEL_PARALLEL_ENABLED=true

# Thread pool size for I/O-bound work (default: 4)
# Handles LLM API calls, file I/O, network operations.
# Higher values help when many operations wait on external services.
# BABEL_IO_WORKERS=4

# Process pool size for CPU-bound work (default: CPU_COUNT / 2)
# Handles text parsing, similarity computation, graph operations.
# Uses half of available cores to leave headroom for system.
# BABEL_CPU_WORKERS=4

# Maximum concurrent LLM API calls (default: 3)
# Limits parallel requests to avoid hitting provider rate limits.
# Adjust based on your API tier (higher tiers can use more).
# BABEL_LLM_CONCURRENT=3

# LLM API rate limit - max requests per second (default: 10.0)
# Additional throttling beyond concurrent limit.
# Lower this if you receive 429 (rate limit) errors.
# BABEL_LLM_RATE_LIMIT=10.0

# Default task timeout in seconds (default: 60)
# Tasks exceeding this timeout will be cancelled.
# LLM calls may need longer timeouts for complex queries.
# BABEL_TASK_TIMEOUT=60

# Shutdown timeout in seconds (default: 10)
# How long to wait for pending tasks during shutdown.
# BABEL_SHUTDOWN_TIMEOUT=10

# Fallback to sequential on pool failure (default: true)
# If true, gracefully degrades to sequential execution.
# If false, raises errors on pool failures.
# BABEL_FALLBACK_SEQUENTIAL=true

# -----------------------------------------------------------------------------
# Skills Installation (set by `babel skill export`)
# -----------------------------------------------------------------------------
#
# Tracks whether Babel skills have been exported to the IDE.
# Updated automatically when running `babel skill export`.
#
# Used by:
#   - `babel prompt --install --auto` to choose mini vs full prompt
#   - LLMs to verify skills are installed
#
# Values:
#   BABEL_SKILLS_INSTALLED: true | false
#   BABEL_SKILLS_TARGET: claude-code | cursor | codex | generic
#
# BABEL_SKILLS_INSTALLED=false
# BABEL_SKILLS_TARGET=
